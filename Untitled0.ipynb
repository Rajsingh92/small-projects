{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-PYc3QYJjgbkBdo0FO6P9bMnxVtls7hl",
      "authorship_tag": "ABX9TyNVWEJC0m6depOy8xqqWuDF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajsingh92/small-projects/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqVtAEHw7ojo",
        "outputId": "2f31ab2f-7e26-4505-bfde-1e6d4364d72e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z518fbIh8B5A"
      },
      "source": [
        "DATA_DIR = \"drive/My Drive/captcha_images_v2\"\n",
        "BATCH_SIZE = 8\n",
        "IMAGE_WIDTH = 300\n",
        "IMAGE_HEIGHT = 75\n",
        "NUM_WORKERS = 8\n",
        "EPOCHS = 200\n",
        "DEVICE = \"cuda\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWxSZKdn8-yi"
      },
      "source": [
        "import numpy as np\n",
        "import  torch\n",
        "import albumentations\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class ClassificationDataset:\n",
        "    def __init__(self,image_paths,targets,resize=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.targets = targets\n",
        "        self.resize = resize\n",
        "        self.aug = albumentations.Compose([albumentations.Normalize(always_apply=True)])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        image = Image.open(self.image_paths[item]).convert(\"RGB\")\n",
        "        targets = self.targets[item]\n",
        "\n",
        "        if self.resize is not None:\n",
        "            image = image.resize(\n",
        "                (self.resize[1],self.resize[0]),resample = Image.BILINEAR\n",
        "                )\n",
        "\n",
        "        image = np.array(image)\n",
        "        augmented = self.aug(image=image)\n",
        "        image = augmented['image']\n",
        "        image = np.transpose(image,(2,0,1)).astype(np.float32)\n",
        "\n",
        "        return {\n",
        "            \"images\":torch.tensor(image,dtype=torch.float),\n",
        "            \"targets\": torch.tensor(targets,dtype=torch.long),\n",
        "        }\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMdlAqHO9Kn_"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch \n",
        "\n",
        "\n",
        "\n",
        "def train_fn(model,data_loader,optimizers):\n",
        "    model.train()\n",
        "    fin_loss = 0 \n",
        "    tk = tqdm(data_loader,total=len(data_loader))\n",
        "\n",
        "    for data in tk:\n",
        "        for k,v in data.items():\n",
        "            data[k] = v.to(DEVICE)\n",
        "        optimizers.zero_grad()\n",
        "        _,loss = model(**data)\n",
        "        loss.backward()\n",
        "        optimizers.step()\n",
        "        fin_loss+=loss.item()\n",
        "\n",
        "    return fin_loss/len(data_loader)\n",
        "\n",
        "def eval_fn(model,data_loader):\n",
        "    model.eval()\n",
        "    fin_loss = 0 \n",
        "    fin_preds = []\n",
        "    tk = tqdm(data_loader,total=len(data_loader))\n",
        "\n",
        "    for data in tk:\n",
        "        for k,v in data.items():\n",
        "            data[k] = v.to(DEVICE)\n",
        "\n",
        "    \n",
        "        batch_preds,loss = model(**data)\n",
        "        fin_loss+= loss.item()\n",
        "        fin_preds.append(batch_preds)\n",
        "\n",
        "    return fin_preds,fin_loss/len(data_loader)\n",
        "      \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zYXC93V9cPJ",
        "outputId": "a39d6eaf-9dde-41be-ecea-90f4d240d698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CaptchaModel(nn.Module):\n",
        "    def __init__(self,num_chars):\n",
        "        super(CaptchaModel,self).__init__()\n",
        "        self.conv_1 = nn.Conv2d(3,128,kernel_size=(3,3),padding=(1,2))\n",
        "        self.max_pool_1 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "\n",
        "        self.conv_2 = nn.Conv2d(128,64,kernel_size=(3,3),padding=(1,2))\n",
        "        self.max_pool_2 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "\n",
        "        self.linear_1 = nn.Linear(1152,64)\n",
        "        self.drop_1 = nn.Dropout(0.2)\n",
        "\n",
        "        self.gru = nn.GRU(64,32,bidirectional = True,num_layers=2,dropout = 0.25)\n",
        "        self.output = nn.Linear(64,num_chars+1)\n",
        "\n",
        "    def forward(self,images,targets=None):\n",
        "        bs,c,h,w = images.size()\n",
        "        #print(bs,c,h,w)\n",
        "        x = F.relu(self.conv_1(images))\n",
        "        #print(x.size())\n",
        "        x = self.max_pool_1(x)\n",
        "        #print(x.size())\n",
        "        x = F.relu(self.conv_2(x))\n",
        "        #print(x.size())\n",
        "        x = self.max_pool_2(x)  #[1, 64, 18, 76]\n",
        "        #print(x.size())\n",
        "        x = x.permute(0,3,1,2)  #[1, 76, 64, 18]\n",
        "        #print(x.size())\n",
        "        x= x.view(bs,x.size(1),-1)\n",
        "        #print(x.size())\n",
        "        x= self.linear_1(x)\n",
        "        x= self.drop_1(x)\n",
        "        #print(x.size())\n",
        "        x,_= self.gru(x)\n",
        "        #print(x.size())\n",
        "        x= self.output(x)\n",
        "        #print(x.size())\n",
        "        x = x.permute(1,0,2)\n",
        "        #print(x.size())\n",
        "        if targets is not None:\n",
        "            log_softmax_values = F.log_softmax(x,2)\n",
        "            input_lengths = torch.full(\n",
        "                size=(bs,),\n",
        "                fill_value=log_softmax_values.size(0),\n",
        "                dtype=torch.int32\n",
        "            )\n",
        "            print(input_lengths)\n",
        "\n",
        "            target_lengths = torch.full(\n",
        "                size=(bs,),\n",
        "                fill_value=log_softmax_values.size(1),\n",
        "                dtype=torch.int32\n",
        "            )\n",
        "            print(target_lengths)\n",
        "            loss =nn.CTCLoss(blank=0)(\n",
        "                log_softmax_values,targets,input_lengths,target_lengths\n",
        "            )\n",
        "            return x,loss\n",
        "\n",
        "        return x,None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cm = CaptchaModel(19)\n",
        "    img = torch.rand(5,3,75,300)\n",
        "    target = torch.randint(1,20,(5,8))\n",
        "    x,loss = cm(img,target)\n",
        "\n",
        "\n",
        "        \n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([76, 76, 76, 76, 76], dtype=torch.int32)\n",
            "tensor([5, 5, 5, 5, 5], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eJA5XAp9k1Z",
        "outputId": "7efe178d-d71c-4a1d-e89c-aca9557951e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.utils.data\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_training():\n",
        "    image_files = glob.glob(os.path.join(DATA_DIR, \"*.png\"))\n",
        "    targets_orig = [x.split(\"/\")[-1][:-4] for x in image_files]\n",
        "    targets = [[c for c in x] for x in targets_orig]\n",
        "    targets_flat = [c for clist in targets for c in clist]\n",
        "\n",
        "    lbl_enc = preprocessing.LabelEncoder()\n",
        "    lbl_enc.fit(targets_flat)\n",
        "    targets_enc = [lbl_enc.transform(x) for x in targets]\n",
        "    targets_enc = np.array(targets_enc)+1\n",
        "\n",
        "    (\n",
        "        train_imgs,\n",
        "        test_imgs,\n",
        "        train_targets,\n",
        "        test_targets,\n",
        "        train_orig_targets,\n",
        "        test_orig_targets\n",
        "    ) = model_selection.train_test_split(image_files,targets_enc,targets_orig,test_size=0.1,random_state=42)\n",
        "\n",
        "    train_dataset = ClassificationDataset(\n",
        "        image_paths=train_imgs, \n",
        "        targets=train_targets, \n",
        "        resize=(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_dataset = ClassificationDataset(\n",
        "        image_paths=train_imgs,\n",
        "        targets=test_targets,\n",
        "        resize=(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    model = CaptchaModel(num_chars=len(lbl_enc.classes_))\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=3e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,factor=0.8,patience=5,verbose=True\n",
        "    )\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train_fn(model,train_loader,optimizer)\n",
        "        valid_preds,valid_loss = eval_fn(model,train_loader)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_training()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/117 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([76, 76, 76, 76, 76, 76, 76, 76], dtype=torch.int32)\n",
            "tensor([8, 8, 8, 8, 8, 8, 8, 8], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8382ecdb9add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-8382ecdb9add>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mvalid_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c35e8a3b1329>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, data_loader, optimizers)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-30729c004b3f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             loss =nn.CTCLoss(blank=0)(\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mlog_softmax_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n\u001b[0;32m-> 1360\u001b[0;31m                           self.zero_infinity)\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[0;31m# TODO: L1HingeEmbeddingCriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \"\"\"\n\u001b[1;32m   2154\u001b[0m     return torch.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, _Reduction.get_enum(reduction),\n\u001b[0;32m-> 2155\u001b[0;31m                           zero_infinity)\n\u001b[0m\u001b[1;32m   2156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have size at least 8 at dimension 1, but got size 5 for argument #2 'targets' (while checking arguments for ctc_loss_gpu)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jsMnHp2-m17"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}